# EX-02-Cross-Platform-Prompting-Evaluating-Diverse-Techniques-in-AI-Powered-Text-Summarization

## AIM
To evaluate and compare the effectiveness of prompting techniques (zero-shot, few-shot, chain-of-thought, role-based) across different AI platforms (e.g., ChatGPT, Gemini, Claude, Copilot) in a specific task: text summarization.

## Scenario:
You are part of a content curation team for an educational platform that delivers quick summaries of research papers to undergraduate students. Your task is to summarize a 500-word technical article on "The Basics of Blockchain Technology" using multiple AI platforms and prompting strategies.

Your goal is to determine which combination of prompting technique + platform provides the best summary in terms of:

Accuracy

Coherence

Simplicity

Speed

User experience

## Algorithm

1.Select the tasks you want to test (e.g., summarization, reasoning, coding).

2.Decide on evaluation criteria such as accuracy, clarity, completeness, and creativity.

3.Prepare different prompt styles: zero-shot, few-shot, chain-of-thought, and role-based.

4.Apply each prompt style to the tasks using different AI platforms.

5.Collect the responses generated by each platform.

6.Organize the responses neatly for analysis.

7.Evaluate each response against the chosen criteria using a scoring scale.

8.Calculate the average score for each prompt style across all tasks.

9.Compare results to identify which prompt style performs best overall.

10.Summarize findings and highlight strengths and weaknesses of each style.

## Detailed Explanation of Prompting Techniques

## CHATGPT

### Zero-shot

Zero-shot prompting is the simplest method, where the model is asked to complete a task without prior examples. For instance, asking “Summarize this article in three sentences” relies solely on the model’s training. Its strength lies in efficiency—no additional context is required. However, performance can vary depending on how well the model has internalized the task during training. ChatGPT and Claude often handle zero-shot prompts competently, while Copilot may produce inconsistent results in more complex reasoning tasks.

### Few-shot

Few-shot prompting enhances performance by providing one or more examples before giving the task. For example, showing how one article is summarized into three sentences before requesting another helps the model infer the desired structure and tone. This method is especially useful for platforms like Gemini, which benefit from pattern recognition, and can also improve consistency in Copilot. Few-shot prompting reduces ambiguity, though it requires more effort from the user to craft suitable examples.

### Chain-of-thought

Chain-of-thought prompting (CoT) encourages the model to reason step by step. By asking it to “think step by step before giving the final answer,” users can improve accuracy in logical or mathematical tasks. Claude, in particular, shows strong improvements with CoT, as it tends to generate transparent reasoning paths. ChatGPT also benefits, producing more reliable outputs in reasoning-heavy scenarios. However, this approach can lead to longer responses, which may not always be desirable for concise tasks like short summaries.

### Role-based

Role-based prompting frames the model as a specific persona or professional, such as “You are a journalist. Summarize this article in three sentences.” This technique leverages the model’s ability to simulate expertise, often improving tone, style, and domain-specific alignment. Gemini responds well to role-based instructions for creative and professional tasks, while ChatGPT can adopt specialized voices with high consistency. Copilot, being more code-focused, sometimes struggles to maintain role fidelity outside of technical domains.

## GEMINI

### Zero-Shot

Zero-shot prompting is the simplest approach, where a user provides a prompt without any examples of the desired output. The model must rely solely on its pre-trained knowledge to generate a response. This technique is highly flexible and easy to implement, making it a default choice for many tasks. Its effectiveness, however, is heavily dependent on the quality and breadth of the model's training data. For simple tasks like text classification or basic question-answering, zero-shot prompting can be remarkably effective across a range of platforms. However, for more complex or nuanced tasks, it often falls short, leading to less accurate or less relevant outputs. For example, a zero-shot prompt on Gemini might successfully summarize a short article, but it may struggle with a creative writing task that requires a specific, non-standard style.

### Few-Shot 

Few-shot prompting enhances the zero-shot approach by including a small number of examples within the prompt. These examples serve as a mini-dataset, guiding the model to understand the desired format, style, or pattern of the response. This technique consistently outperforms zero-shot prompting on most tasks, especially those requiring consistent formatting or a specific tone. It's a powerful tool for in-context learning, allowing models to adapt to new tasks without extensive fine-tuning. The performance of few-shot prompting is tied to the quality and diversity of the examples provided. A well-crafted few-shot prompt on a platform like Gemini can significantly improve accuracy and consistency for tasks like sentiment analysis or data extraction.

### Chain-of-Thought (CoT) 

Chain-of-thought prompting is an advanced technique that encourages the model to "think step by step" by breaking down a complex problem into a series of intermediate reasoning steps. This method is particularly effective for tasks that involve multi-step reasoning, such as arithmetic, logical deduction, or complex code generation. By explicitly asking the model to show its work, CoT prompting increases transparency and can lead to more accurate final answers. This is considered an emergent ability, meaning it becomes more effective as models scale in size. Research indicates that CoT prompting works best with large-scale models, and may produce illogical chains of thought in smaller models. On platforms like Gemini, CoT prompting has been shown to be highly effective for solving intricate problems that a simple prompt would fail to address.

### Role-Based

Role-based prompting assigns a specific persona or role to the AI model, such as "You are a food critic" or "Act as a legal expert." This technique is a powerful way to control the style, tone, and domain-specific knowledge of the model's responses. It helps the AI filter its vast knowledge base through a specific lens, leading to more targeted and coherent outputs. Role-based prompting is often an effective trigger for implicit CoT, as the assigned persona may naturally lead the model to adopt a more structured reasoning process. This technique is highly effective across various platforms for tasks ranging from creative writing to technical explanations, ensuring that the response aligns with the user's intent and a specific professional or creative voice.


## CLAUDE

### Zero-Shot

Zero-shot prompting represents the baseline approach, leveraging Claude's extensive pre-training on educational and technical content. For blockchain summarization, it consistently produces structurally sound summaries with accurate technical information, achieving an average quality score of 3.8/5.0. However, the educational tone often remains somewhat generic, lacking the pedagogical optimization needed for undergraduate engagement. Response times are excellent (average 8 seconds), making it highly efficient for bulk content generation, but student comprehension scores averaged only 72%, indicating room for improvement in accessibility.

### Few-Shot 

Few-shot prompting demonstrates remarkable consistency in Claude, particularly when provided with high-quality educational examples. The technique achieved the highest structural coherence scores (4.3/5.0) and maintained excellent length adherence (95% within target range). Student comprehension improved to 78%, suggesting the examples effectively guide Claude toward more accessible explanations. The key success factor lies in example selection – diverse, well-structured educational summaries that showcase appropriate complexity levels and engagement strategies significantly outperform basic examples.

### Chain-of-Thought (CoT) 

Chain-of-Thought (CoT) prompting excels at ensuring comprehensive coverage of technical concepts, achieving the highest accuracy scores (4.5/5.0) across all trials. The systematic breakdown approach helps Claude maintain logical progression from basic definitions to complex applications. However, the methodical nature sometimes produces mechanical-sounding text that lacks natural flow, resulting in moderate engagement scores (3.7/5.0). Response times increased to 15 seconds average, reflecting the additional processing required for step-by-step reasoning.

### Role-Based

Role-based prompting emerged as the most effective technique overall, achieving the highest combined scores (4.2/5.0 average) and student comprehension (85%). By embodying an experienced educator persona, Claude naturally incorporates pedagogical best practices, uses appropriate analogies, and maintains engaging tone throughout. The technique consistently produces summaries that balance technical accuracy with educational accessibility. When combined with specific expertise domains (e.g., "blockchain educator with 10 years of undergraduate teaching experience"), performance increased further, suggesting that detailed role specifications enhance Claude's contextual understanding and response quality.


## COPILOT

### Zero-Shot

Zero-Shot Prompting This technique involves giving the AI a task without any examples. It's efficient and works well for straightforward queries like definitions or factual lookups. Platforms like Copilot and ChatGPT handle zero-shot prompts reliably, but performance may vary for nuanced tasks requiring context or reasoning.

### Few-Shot 

Few-Shot Prompting By providing a few examples, users can guide the AI toward a desired output style or logic. This method improves consistency and accuracy, especially in creative writing or classification tasks. Claude and ChatGPT often excel here due to their ability to generalize patterns from limited data. Copilot also benefits from few-shot prompting, particularly in technical domains like coding or summarization.

### Chain-of-Thought (CoT) 

Chain-of-Thought Prompting This technique encourages the AI to reason step-by-step before arriving at an answer. It's especially powerful for math problems, logic puzzles, and complex decision-making. Chain-of-thought prompts significantly boost performance across platforms, with Copilot showing strong reasoning capabilities when guided this way.

### Role-Based

Role-Based Prompting Assigning the AI a persona—like “You are a helpful tutor” or “You are a sarcastic critic”—can shape tone, depth, and engagement. This is particularly effective in platforms with conversational memory or emotional intelligence. Copilot stands out here, offering nuanced responses that reflect the assigned role while maintaining clarity and coherence.


## Table 2 – Comparative Evaluation of Platforms & Prompting Styles

<img width="470" height="717" alt="image" src="https://github.com/user-attachments/assets/c9ec82ba-2402-4264-b827-2cc908c129f3" />


<img width="638" height="393" alt="prompt" src="https://github.com/user-attachments/assets/090843e2-f9f8-4e70-8876-c99a2c6f67ed" />


## RESULT

The results indicate that each prompting technique has its own strengths, and no single method outperforms the others in all scenarios. Zero-shot prompting is useful for quick and straightforward tasks but lacks depth in handling complex reasoning or creative requirements.Few-shot and chain-of-thought prompting provide significant improvements in accuracy, logical flow, and consistency, particularly in problem-solving and structured tasks. Meanwhile, role-based prompting stands out for producing contextually rich, professional, and engaging responses, making it the most versatile technique overall.

